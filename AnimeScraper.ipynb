{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://myanimelist.net/topanime.php\"\n",
    "# r = requests.get(URL)\n",
    "# soup = BeautifulSoup(r.content, 'html5lib')\n",
    "# print(soup.prettify())\n",
    "\n",
    "# Number of results per page\n",
    "results_per_page = 50\n",
    "\n",
    "# Number of pages to scrape (in this case, 7 pages for 350 results)\n",
    "num_pages = 10\n",
    "animes = []\n",
    "for page in range(num_pages):\n",
    "    # Calculate the offset for each page\n",
    "    offset = page * results_per_page\n",
    "\n",
    "    # Prepare the query parameters with the appropriate offset\n",
    "    params = {\n",
    "        \"limit\": offset\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the page with the query parameters\n",
    "    response = requests.get(URL, params=params)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "\n",
    "    # Perform scraping operations on the current page\n",
    "    \n",
    "    for tr in soup.find_all('tr',attrs={'class':\"ranking-list\"}):\n",
    "        anime = {}\n",
    "        for td in tr.findAll('td',attrs={'class':'rank ac'}):\n",
    "            for tag in td.span:\n",
    "                anime['Rank'] = tag.string\n",
    "        for td in tr.find_all('td',attrs={'class':'title al va-t word-break'}):\n",
    "            for tag in td.h3:\n",
    "                anime['Name'] = tag.string\n",
    "        for td in tr.findAll('td',attrs={'class':'score ac fs14'}):\n",
    "            for tag in td.span:\n",
    "                anime['Score'] = tag.string\n",
    "        for td in tr.find_all('td', attrs={'class': 'title al va-t word-break'}):\n",
    "        # Extract the number of anime episodes\n",
    "            episodes = td.find('div', class_='information di-ib mt4').contents[0].strip()\n",
    "            \n",
    "            # Extract the date\n",
    "            date = td.find('div', class_='information di-ib mt4').contents[2].strip()\n",
    "            \n",
    "            # Extract the number of members\n",
    "            members = td.find('div', class_='information di-ib mt4').contents[4].strip()\n",
    "            \n",
    "            anime['Episodes'] = episodes\n",
    "            anime['Date'] = date\n",
    "            anime['Members'] = members\n",
    "            animes.append(anime)\n",
    "    \n",
    "    # Wait for a few seconds before scraping the next page (to be respectful to the server)\n",
    "    # time.sleep(2)\n",
    "\n",
    "animes_df = pd.DataFrame(animes)\n",
    "animes_df.to_csv('Anime List.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "animes = []\n",
    "for tr in soup.find_all('tr',attrs={'class':\"ranking-list\"}):\n",
    "    anime = {}\n",
    "    for td in tr.findAll('td',attrs={'class':'rank ac'}):\n",
    "        for tag in td.span:\n",
    "            anime['Rank'] = tag.string\n",
    "    for td in tr.find_all('td',attrs={'class':'title al va-t word-break'}):\n",
    "        for tag in td.h3:\n",
    "            anime['Name'] = tag.string\n",
    "    for td in tr.findAll('td',attrs={'class':'score ac fs14'}):\n",
    "        for tag in td.span:\n",
    "            anime['Score'] = tag.string\n",
    "    for td in tr.find_all('td', attrs={'class': 'title al va-t word-break'}):\n",
    "    # Extract the number of anime episodes\n",
    "        episodes = td.find('div', class_='information di-ib mt4').contents[0].strip()\n",
    "        \n",
    "        # Extract the date\n",
    "        date = td.find('div', class_='information di-ib mt4').contents[2].strip()\n",
    "        \n",
    "        # Extract the number of members\n",
    "        members = td.find('div', class_='information di-ib mt4').contents[4].strip()\n",
    "        \n",
    "        anime['Episodes'] = episodes\n",
    "        anime['Date'] = date\n",
    "        anime['Members'] = members\n",
    "        animes.append(anime)\n",
    "animes\n",
    "\n",
    "animes_df = pd.DataFrame(animes)\n",
    "animes_df.to_csv('AnimeList.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fullmetal Alchemist\n",
      "GintamaÂ°: Umai-mono wa Atomawashi ni Suru to Yokodorisareru kara Yappari Saki ni Kue\n",
      "Honzuki no Gekokujou: Shisho ni Naru Tame ni wa Shudan wo Erandeiraremasen 2nd Season\n",
      "Akatsuki no Yona OVA\n",
      "JoJo no Kimyou na Bouken Part 3: Stardust Crusaders\n",
      "Kidou Senshi Gundam Unicorn\n",
      "Kuroko no Basket: Saikou no Present desu\n",
      "Mirai Shounen Conan\n",
      "Tamayura: Sotsugyou Shashin Part 4 - Ashita\n",
      "Tsubasa: Shunraiki\n",
      "Zankyou no Terror\n",
      "Tsukimonogatari\n",
      "Tsurune: Tsunagari no Issha\n",
      "Uchouten Kazoku 2\n",
      "Ansatsu Kyoushitsu\n",
      "Baccano! Specials\n",
      "Fumetsu no Anata e Season 2\n",
      "Gin no Saji\n",
      "Hajime no Ippo: Boxer no Kobushi\n",
      "Initial D Fifth Stage\n",
      "Kamisama Hajimemashita\n",
      "Mahoutsukai no Yome Season 2\n",
      "One Piece Film: Strong World\n",
      "Overlord IV\n",
      "Saint Seiya: The Lost Canvas - Meiou Shinwa 2\n",
      "Sakura-sou no Pet na Kanojo\n",
      "Shokugeki no Souma: Ni no Sara\n",
      "Stand By Me Doraemon\n",
      "Kidou Senshi Gundam 00 Second Season\n",
      "Komi-san wa, Comyushou desu. 2nd Season\n",
      "Kuroko no Basket Movie 4: Last Game\n",
      "No Game No Life\n",
      "Romantic Killer\n",
      "Serial Experiments Lain\n",
      "Skip Beat!\n",
      "Tonikaku Kawaii: SNS\n",
      "Toradora!\n",
      "5-toubun no Hanayome âˆ¬\n",
      "Ginga Eiyuu Densetsu: Arata naru Tatakai no Overture\n",
      "Gintama: Nanigoto mo Saisho ga Kanjin nanode Tashou Senobisuru Kurai ga Choudoyoi\n",
      "Hikaru no Go\n",
      "Honzuki no Gekokujou: Shisho ni Naru Tame ni wa Shudan wo Erandeiraremasen 3rd Season\n",
      "Hyouka\n",
      "Kanata no Astra\n",
      "Detective Conan Movie 18: The Sniper from Another Dimension\n",
      "Gintama: Dai Hanseikai\n",
      "Hidamari Sketch: Sae Hiro Sotsugyou-hen\n",
      "Koukaku Kidoutai: Stand Alone Complex - The Laughing Man\n",
      "Kuroshitsuji: Book of Circus\n",
      "Yowamushi Pedal: Grande Road\n"
     ]
    }
   ],
   "source": [
    "table = soup.find_all('div', attrs={'class':'di-ib clearfix'})\n",
    "soup.table.h3.a.string\n",
    "\n",
    "for name in table:\n",
    "    print(name.h3.a.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "img\n",
      "div\n",
      "div\n",
      "div\n",
      "div\n",
      "h3\n",
      "a\n",
      "div\n",
      "a\n",
      "i\n",
      "br\n",
      "div\n",
      "br\n",
      "br\n",
      "div\n",
      "a\n",
      "i\n",
      "span\n",
      "span\n",
      "span\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# <td class=\"title al va-t word-break\">\n",
    "detail = soup.find_all('td',attrs={'class':'title al va-t word-break'})\n",
    "detail = detail[0]\n",
    "for tag in detail.find_all():\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = []\n",
    "for td in soup.find_all('td',attrs={'class':'title al va-t word-break'}):\n",
    "    for div in td.findAll('div',attrs={'class':'information di-ib mt4'}):\n",
    "        mylist.append(div)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: TV (51 eps)\n",
      "Date: Oct 2003 - Oct 2004\n",
      "Members: 1,422,693 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Nov 2015 - Nov 2015\n",
      "Members: 33,041 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Apr 2020 - Jun 2020\n",
      "Members: 191,266 members\n",
      "#########\n",
      "Episodes: OVA (3 eps)\n",
      "Date: Sep 2015 - Dec 2016\n",
      "Members: 128,913 members\n",
      "#########\n",
      "Episodes: TV (24 eps)\n",
      "Date: Apr 2014 - Sep 2014\n",
      "Members: 1,141,273 members\n",
      "#########\n",
      "Episodes: OVA (7 eps)\n",
      "Date: Mar 2010 - Jun 2014\n",
      "Members: 95,013 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Dec 2015 - Dec 2015\n",
      "Members: 101,228 members\n",
      "#########\n",
      "Episodes: TV (26 eps)\n",
      "Date: Apr 1978 - Oct 1978\n",
      "Members: 47,251 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Apr 2016 - Apr 2016\n",
      "Members: 10,632 members\n",
      "#########\n",
      "Episodes: OVA (2 eps)\n",
      "Date: Mar 2009 - May 2009\n",
      "Members: 60,822 members\n",
      "#########\n",
      "Episodes: TV (11 eps)\n",
      "Date: Jul 2014 - Sep 2014\n",
      "Members: 1,112,669 members\n",
      "#########\n",
      "Episodes: TV (4 eps)\n",
      "Date: Dec 2014 - Dec 2014\n",
      "Members: 403,711 members\n",
      "#########\n",
      "Episodes: TV (13 eps)\n",
      "Date: Jan 2023 - Mar 2023\n",
      "Members: 42,986 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Apr 2017 - Jun 2017\n",
      "Members: 62,178 members\n",
      "#########\n",
      "Episodes: TV (22 eps)\n",
      "Date: Jan 2015 - Jun 2015\n",
      "Members: 1,905,377 members\n",
      "#########\n",
      "Episodes: Special (3 eps)\n",
      "Date: Feb 2008 - May 2008\n",
      "Members: 174,765 members\n",
      "#########\n",
      "Episodes: TV (20 eps)\n",
      "Date: Oct 2022 - Mar 2023\n",
      "Members: 239,889 members\n",
      "#########\n",
      "Episodes: TV (11 eps)\n",
      "Date: Jul 2013 - Sep 2013\n",
      "Members: 283,014 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Mar 2003 - Mar 2003\n",
      "Members: 51,332 members\n",
      "#########\n",
      "Episodes: TV (14 eps)\n",
      "Date: Nov 2012 - May 2013\n",
      "Members: 137,150 members\n",
      "#########\n",
      "Episodes: TV (13 eps)\n",
      "Date: Oct 2012 - Dec 2012\n",
      "Members: 619,499 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Apr 2023 - Jun 2023\n",
      "Members: 123,163 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Dec 2009 - Dec 2009\n",
      "Members: 238,264 members\n",
      "#########\n",
      "Episodes: TV (13 eps)\n",
      "Date: Jul 2022 - Sep 2022\n",
      "Members: 454,623 members\n",
      "#########\n",
      "Episodes: OVA (13 eps)\n",
      "Date: Feb 2011 - Jul 2011\n",
      "Members: 72,845 members\n",
      "#########\n",
      "Episodes: TV (24 eps)\n",
      "Date: Oct 2012 - Mar 2013\n",
      "Members: 1,201,921 members\n",
      "#########\n",
      "Episodes: TV (13 eps)\n",
      "Date: Jul 2016 - Sep 2016\n",
      "Members: 1,092,338 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Aug 2014 - Aug 2014\n",
      "Members: 50,647 members\n",
      "#########\n",
      "Episodes: TV (25 eps)\n",
      "Date: Oct 2008 - Mar 2009\n",
      "Members: 146,454 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Apr 2022 - Jun 2022\n",
      "Members: 426,620 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Mar 2017 - Mar 2017\n",
      "Members: 284,728 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Apr 2014 - Jun 2014\n",
      "Members: 2,297,634 members\n",
      "#########\n",
      "Episodes: ONA (12 eps)\n",
      "Date: Oct 2022 - Oct 2022\n",
      "Members: 149,611 members\n",
      "#########\n",
      "Episodes: TV (13 eps)\n",
      "Date: Jul 1998 - Sep 1998\n",
      "Members: 714,071 members\n",
      "#########\n",
      "Episodes: TV (25 eps)\n",
      "Date: Oct 2008 - Mar 2009\n",
      "Members: 260,541 members\n",
      "#########\n",
      "Episodes: OVA (1 eps)\n",
      "Date: Aug 2021 - Aug 2021\n",
      "Members: 137,209 members\n",
      "#########\n",
      "Episodes: TV (25 eps)\n",
      "Date: Oct 2008 - Mar 2009\n",
      "Members: 2,113,490 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Jan 2021 - Mar 2021\n",
      "Members: 660,187 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Dec 1993 - Dec 1993\n",
      "Members: 33,052 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Sep 2005 - Sep 2005\n",
      "Members: 54,618 members\n",
      "#########\n",
      "Episodes: TV (75 eps)\n",
      "Date: Oct 2001 - Mar 2003\n",
      "Members: 133,200 members\n",
      "#########\n",
      "Episodes: TV (10 eps)\n",
      "Date: Apr 2022 - Jun 2022\n",
      "Members: 134,250 members\n",
      "#########\n",
      "Episodes: TV (22 eps)\n",
      "Date: Apr 2012 - Sep 2012\n",
      "Members: 1,310,000 members\n",
      "#########\n",
      "Episodes: TV (12 eps)\n",
      "Date: Jul 2019 - Sep 2019\n",
      "Members: 286,293 members\n",
      "#########\n",
      "Episodes: Movie (1 eps)\n",
      "Date: Apr 2014 - Apr 2014\n",
      "Members: 41,023 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Mar 2010 - Mar 2010\n",
      "Members: 36,793 members\n",
      "#########\n",
      "Episodes: OVA (2 eps)\n",
      "Date: Nov 2013 - Nov 2013\n",
      "Members: 13,362 members\n",
      "#########\n",
      "Episodes: Special (1 eps)\n",
      "Date: Sep 2005 - Sep 2005\n",
      "Members: 39,397 members\n",
      "#########\n",
      "Episodes: TV (10 eps)\n",
      "Date: Jul 2014 - Sep 2014\n",
      "Members: 385,951 members\n",
      "#########\n",
      "Episodes: TV (24 eps)\n",
      "Date: Oct 2014 - Mar 2015\n",
      "Members: 134,660 members\n",
      "#########\n"
     ]
    }
   ],
   "source": [
    "for td in soup.find_all('td', attrs={'class': 'title al va-t word-break'}):\n",
    "    # Extract the number of anime episodes\n",
    "    episodes = td.find('div', class_='information di-ib mt4').contents[0].strip()\n",
    "    \n",
    "    # Extract the date\n",
    "    date = td.find('div', class_='information di-ib mt4').contents[2].strip()\n",
    "    \n",
    "    # Extract the number of members\n",
    "    members = td.find('div', class_='information di-ib mt4').contents[4].strip()\n",
    "    \n",
    "    print(\"Episodes:\", episodes)\n",
    "    print(\"Date:\", date)\n",
    "    print(\"Members:\", members)\n",
    "    print(\"#########\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
