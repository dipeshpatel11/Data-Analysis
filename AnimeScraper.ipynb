{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://myanimelist.net/topanime.php\"\n",
    "# r = requests.get(URL)\n",
    "# soup = BeautifulSoup(r.content, 'html5lib')\n",
    "# print(soup.prettify())\n",
    "\n",
    "# Number of results per page\n",
    "results_per_page = 50\n",
    "\n",
    "# Number of pages to scrape (in this case, 7 pages for 350 results)\n",
    "num_pages = 10\n",
    "animes = []\n",
    "for page in range(num_pages):\n",
    "    # Calculate the offset for each page\n",
    "    offset = page * results_per_page\n",
    "\n",
    "    # Prepare the query parameters with the appropriate offset\n",
    "    params = {\n",
    "        \"limit\": offset\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the page with the query parameters\n",
    "    response = requests.get(URL, params=params)\n",
    "    html_content = response.content\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, \"html5lib\")\n",
    "\n",
    "    # Perform scraping operations on the current page\n",
    "    \n",
    "    for tr in soup.find_all('tr',attrs={'class':\"ranking-list\"}):\n",
    "        anime = {}\n",
    "        for td in tr.findAll('td',attrs={'class':'rank ac'}):\n",
    "            for tag in td.span:\n",
    "                anime['Rank'] = tag.string\n",
    "        for td in tr.find_all('td',attrs={'class':'title al va-t word-break'}):\n",
    "            for tag in td.h3:\n",
    "                anime['Name'] = tag.string\n",
    "        for td in tr.findAll('td',attrs={'class':'score ac fs14'}):\n",
    "            for tag in td.span:\n",
    "                anime['Score'] = tag.string\n",
    "        for td in tr.find_all('td', attrs={'class': 'title al va-t word-break'}):\n",
    "        # Extract the number of anime episodes\n",
    "            episodes = td.find('div', class_='information di-ib mt4').contents[0].strip()\n",
    "            \n",
    "            # Extract the date\n",
    "            date = td.find('div', class_='information di-ib mt4').contents[2].strip()\n",
    "            \n",
    "            # Extract the number of members\n",
    "            members = td.find('div', class_='information di-ib mt4').contents[4].strip()\n",
    "            \n",
    "            anime['Episodes'] = episodes\n",
    "            anime['Date'] = date\n",
    "            anime['Members'] = members\n",
    "            animes.append(anime)\n",
    "    \n",
    "    # Wait for a few seconds before scraping the next page (to be respectful to the server)\n",
    "    # time.sleep(2)\n",
    "\n",
    "animes_df = pd.DataFrame(animes)\n",
    "animes_df.to_csv('Anime List.csv', index=False)\n"
   ]
  },
 
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
